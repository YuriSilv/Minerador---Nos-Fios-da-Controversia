{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f1dea-7f05-4e1a-b735-731bae9529b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from http import client\n",
    "from datetime import date, datetime\n",
    "import os\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ae15a-9a76-42a1-b88b-1043cf19d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = 'XXXXXXXXXXX'\n",
    "CONSUMER_SECRET = 'XXXXXXXXXXX'\n",
    "BEARER_TOKEN = 'XXXXXXXXXXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239bfa0-23a6-402f-896b-162c4e6f6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto de autenticação\n",
    "client = tweepy.Client(consumer_key = CONSUMER_KEY, consumer_secret = CONSUMER_SECRET, bearer_token=BEARER_TOKEN, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6462e17e-c11a-4aed-a8a7-aa31364fad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funções\n",
    "\n",
    "def drop_dp(json_file):\n",
    "    df = pd.read_json(json_file)\n",
    "    df.drop_duplicates(['texto'], inplace=True)\n",
    "    with open(json_file, \"a+\") as output:\n",
    "        df.to_json(json_file,orient=\"records\")\n",
    "        \n",
    "def json_serial(obj):\n",
    "    if isinstance(obj, (datetime, date)):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError (\"Type %s not serializable\" % type(obj))\n",
    "\n",
    "\n",
    "def filter_query(text, lingua):\n",
    "    if lingua == 'pt':\n",
    "        query_set = {'violência', 'leis rígidas', 'leis', 'aborto', 'agressores', 'racismo', 'ladrão', 'bandido', 'feminicídio', 'maioridade penal'}\n",
    "    elif lingua == 'es':\n",
    "        query_set = {'racismo', 'gallina amarga', 'tema suarez', 'goleados y golpeados', 'cosas de hombres', 'símios que se hacen llamar hinchas'}\n",
    "  \n",
    "    tokens = text.split()\n",
    "    for count,element in enumerate(tokens):\n",
    "        if element in query_set:\n",
    "            return True\n",
    "        if count == len(tokens) - 1:\n",
    "            return False\n",
    "\n",
    "def truncate_utf8_chars(filename, count, ignore_newlines=True):\n",
    "    \"\"\"\n",
    "    Truncates last `count` characters of a text file encoded in UTF-8.\n",
    "    :param filename: The path to the text file to read\n",
    "    :param count: Number of UTF-8 characters to remove from the end of the file\n",
    "    :param ignore_newlines: Set to true, if the newline character at the end of the file should be ignored\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb+') as f:\n",
    "\n",
    "        size = os.fstat(f.fileno()).st_size\n",
    "\n",
    "        offset = 1\n",
    "        chars = 0\n",
    "        while offset <= size:\n",
    "            f.seek(-offset, os.SEEK_END)\n",
    "            b = ord(f.read(1))\n",
    "\n",
    "            if ignore_newlines:\n",
    "                if b == 0x0D or b == 0x0A:\n",
    "                    offset += 1\n",
    "                    continue\n",
    "\n",
    "            if b & 0b10000000 == 0 or b & 0b11000000 == 0b11000000:\n",
    "                # This is the first byte of a UTF8 character\n",
    "                chars += 1\n",
    "                if chars == count:\n",
    "                    # When `count` number of characters have been found, move current position back\n",
    "                    # with one byte (to include the byte just checked) and truncate the file\n",
    "                    f.seek(-1, os.SEEK_CUR)\n",
    "                    f.truncate()\n",
    "                    return\n",
    "            offset += 1\n",
    "            \n",
    "def pesquisa_tweets(query_str, data_json, i_time, f_time):\n",
    "    tweets = client.search_all_tweets(query=query_str, max_results = 500, tweet_fields=[\"created_at\",\"author_id\",\"public_metrics\",\"referenced_tweets\"], start_time = i_time, end_time = f_time)\n",
    "    if (tweets.data != None):\n",
    "        print(f\"Enter\\n\")\n",
    "        for tw in tweets.data:\n",
    "            tweet_autor = tw[\"author_id\"]\n",
    "            tweet_text = tw[\"text\"]\n",
    "            tweet_data = tw[\"created_at\"]\n",
    "            tweet_id = tw[\"id\"]\n",
    "            tweet_reply_count = tw[\"public_metrics\"][\"reply_count\"]\n",
    "            tweet_retweet_count = tw[\"public_metrics\"][\"retweet_count\"]\n",
    "            tweet_like_count = tw[\"public_metrics\"][\"like_count\"]\n",
    "            with open(data_json, \"a+\") as output:\n",
    "                data = {\n",
    "                \"autor\":tweet_autor,\n",
    "                \"tw_id\":tweet_id,\n",
    "                \"texto\":tweet_text,\n",
    "                \"likes\":tweet_like_count,\n",
    "                \"replys\":tweet_reply_count,\n",
    "                \"retweets\": tweet_retweet_count,\n",
    "                \"data\":json_serial(tweet_data)}\n",
    "\n",
    "                output.write(f\"{json.dumps(data)},\")\n",
    "\n",
    "def add_pesquisa(query_add, cidade_dict, data_json, i_time, f_time, is_from = True):\n",
    "    truncate_utf8_chars(data_json, 1)\n",
    "    with open(data_json, \"a+\") as output:\n",
    "        output.write(\",\")\n",
    "    \n",
    "    if is_from == True:\n",
    "        for i in cidade_dict.values():\n",
    "            query =  f'(from:{i}) '\n",
    "            x_query = ''.join((query,query_add))\n",
    "            pesquisa_tweets(x_query, data_json, i_time, f_time)\n",
    "    else:\n",
    "        query = ''\n",
    "        x_query = ''.join((query,query_add))\n",
    "        pesquisa_tweets(x_query, data_json, i_time, f_time)\n",
    "\n",
    "    truncate_utf8_chars(data_json, 1)\n",
    "    with open(data_json, \"a+\") as output:\n",
    "        output.write(\"]\")\n",
    "\n",
    "#pesquisa de likes por id\n",
    "def pesquisa_likes(cidade_dict,data_like_json,lingua):\n",
    "    truncate_utf8_chars(data_like_json, 1)\n",
    "    with open(data_like_json, \"a+\") as output:\n",
    "        output.write(\",\")\n",
    "\n",
    "    for j in cidade_dict.values():\n",
    "        user_like = client.get_liked_tweets(id=f'{j}', max_results = 100, tweet_fields=[\"created_at\",\"author_id\",\"public_metrics\"])\n",
    "        \n",
    "        try:\n",
    "            for like in user_like.data:\n",
    "                if(filter_query(like['text'],lingua)):\n",
    "                    tweet_autor_id = like[\"author_id\"]\n",
    "                    tweet_liked = like[\"text\"]\n",
    "                    tweet_like_data = like[\"created_at\"]\n",
    "                    tweet_like_reply_count = like[\"public_metrics\"][\"reply_count\"]\n",
    "                    tweet_like_retweet_count = like[\"public_metrics\"][\"retweet_count\"]\n",
    "                    tweet_like_likes_count = like[\"public_metrics\"][\"like_count\"]\n",
    "\n",
    "                    with open(data_like_json, \"a+\") as output:\n",
    "                        data_like = {\n",
    "                        \"autor\":tweet_autor_id,\n",
    "                        \"texto\":tweet_liked,\n",
    "                        \"likes\":tweet_like_likes_count,\n",
    "                        \"retweets\":tweet_like_retweet_count,\n",
    "                        \"replys\":tweet_like_reply_count,\n",
    "                        \"data\":json_serial(tweet_like_data)}\n",
    "                        output.write(f\"{json.dumps(data_like)},\\n\")\n",
    "                else:\n",
    "                    continue\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    truncate_utf8_chars(data_like_json, 1)\n",
    "    with open(data_like_json, \"a+\") as output:\n",
    "        output.write(\"]\")\n",
    "\n",
    "def pesquisar_mention(metion_json, data_json, s_time, e_time):\n",
    "    \n",
    "    truncate_utf8_chars(metion_json, 1)\n",
    "    with open(metion_json, \"a+\") as output:\n",
    "        output.write(\",\")\n",
    "    \n",
    "    with open(data_json) as json_file:\n",
    "        data_std = json.loads(json_file.read())\n",
    "\n",
    "        for usr in range(len(data_std)):\n",
    "            ids_data = data_std[usr]['tw_id']\n",
    "            mes_antigo = get_month_time(data_std[usr]['data'])\n",
    "            mes_atual = get_month_time(s_time)\n",
    "            if (mes_antigo == mes_atual):\n",
    "                users_mention = client.search_all_tweets(query=f\"(is:reply) (conversation_id:{data_std[usr]['tw_id']})\",\n",
    "                                                         start_time = s_time, end_time = e_time,\n",
    "                                                         max_results = 100, tweet_fields=[\"author_id\",\"public_metrics\"])\n",
    "\n",
    "                if users_mention.data != None:\n",
    "                    print(\"Processando...\")\n",
    "                    for mention in users_mention.data:\n",
    "                        mention_text = mention[\"text\"]\n",
    "                        mention_reply_count = mention[\"public_metrics\"][\"reply_count\"]\n",
    "                        mention_retweet_count = mention[\"public_metrics\"][\"retweet_count\"]\n",
    "                        mention_like_count = mention[\"public_metrics\"][\"like_count\"]\n",
    "\n",
    "                        with open(metion_json, \"a+\") as output_mention:\n",
    "                            data_mention = {\n",
    "                            \"texto\":mention_text,\n",
    "                            \"tw_mencionado\":data_std[usr]['texto'],\n",
    "                            \"tw_likes\":mention_like_count,\n",
    "                            \"tw_retweet\":mention_retweet_count,\n",
    "                            \"tw_replys\":mention_reply_count}\n",
    "                            output_mention.write(f\"{json.dumps(data_mention)},\\n\")\n",
    "                else:\n",
    "                    print(\"Vazio\")\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    truncate_utf8_chars(metion_json, 1)\n",
    "    with open(metion_json, \"a+\") as output:\n",
    "        output.write(\"]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
